"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç—É ascale.es - –ö–µ—Ä–∞–º–æ–≥—Ä–∞–Ω—ñ—Ç
–ó–±–∏—Ä–∞—î –¥–∞–Ω—ñ –∑ –∫–æ–ª–µ–∫—Ü—ñ–π –±—Ä–µ–Ω–¥—É Ascale
–†–µ–∑—É–ª—å—Ç–∞—Ç: ascale_ceramic.xlsx
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
import time
from tqdm import tqdm
import os
import json

class AscaleParser:
    def __init__(self, output_file='ascale_ceramic.xlsx'):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.output_file = output_file
        self.progress_file = 'progress_ascale.json'
        self.driver = None
        self.wait = None
        self.processed_urls = set()
        self.brand = 'Ascale'
        self.category = '–ö–µ—Ä–∞–º–æ–≥—Ä–∞–Ω—ñ—Ç'
        self.init_driver()
        self.load_progress()
        
    def init_driver(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –¥—Ä–∞–π–≤–µ—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
        options = webdriver.ChromeOptions()
        # –ó–∞–∫–æ–º–µ–Ω—Ç—É–π—Ç–µ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—è–¥–æ–∫, —è–∫—â–æ —Ö–æ—á–µ—Ç–µ –±–∞—á–∏—Ç–∏ –±—Ä–∞—É–∑–µ—Ä
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--lang=en-US')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.wait = WebDriverWait(self.driver, 15)
            print("‚úÖ –ë—Ä–∞—É–∑–µ—Ä —É—Å–ø—ñ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–æ")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø—É—Å–∫—É –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            raise
    
    def load_progress(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø—Ä–æ–≥—Ä–µ—Å –∑ —Ñ–∞–π–ª—É"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –ø—Ä–æ–≥—Ä–µ—Å: {len(self.processed_urls)} —Ç–æ–≤–∞—Ä—ñ–≤ –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω–æ")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—å –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å: {e}")
                self.processed_urls = set()
        else:
            print("üÜï –ü–æ—á–∞—Ç–æ–∫ –Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É")
    
    def save_progress(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–≥—Ä–µ—Å —É —Ñ–∞–π–ª"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls)
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É: {e}")
    
    def restart_driver(self):
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –¥—Ä–∞–π–≤–µ—Ä–∞ –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö"""
        print("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞...")
        try:
            if self.driver:
                self.driver.quit()
        except:
            pass
        
        time.sleep(3)
        self.init_driver()
    
    def get_collection_urls(self, main_url):
        """–û—Ç—Ä–∏–º—É—î URL –≤—Å—ñ—Ö –∫–æ–ª–µ–∫—Ü—ñ–π"""
        print("üåê –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≥–æ–ª–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ–π...")
        
        try:
            self.driver.get(main_url)
            time.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É, —â–æ–± –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # –®—É–∫–∞—î–º–æ –≤—Å—ñ –±–ª–æ–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ–π
            collection_blocks = soup.find_all('div', class_='jet-listing-grid__item')
            
            collections = []
            for block in collection_blocks:
                try:
                    # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∫–æ–ª–µ–∫—Ü—ñ—é
                    link = block.find('a', {'data-element_type': 'container'})
                    if link and link.get('href'):
                        collection_url = link['href']
                        
                        # –ù–∞–∑–≤–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
                        heading = block.find('h3', class_='elementor-heading-title')
                        collection_name = heading.text.strip() if heading else ''
                        
                        collections.append({
                            'name': collection_name,
                            'url': collection_url
                        })
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –±–ª–æ–∫—É –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                    continue
            
            print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–µ–∫—Ü—ñ–π: {len(collections)}")
            return collections
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ–π: {e}")
            return []
    
    def parse_collection_page(self, collection_url, collection_name):
        """–ü–∞—Ä—Å–∏—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É –∫–æ–ª–µ–∫—Ü—ñ—ó —Ç–∞ –æ—Ç—Ä–∏–º—É—î –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏"""
        print(f"\nüìÇ –û–±—Ä–æ–±–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó: {collection_name}")
        
        try:
            self.driver.get(collection_url)
            time.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # –®—É–∫–∞—î–º–æ –≤—Å—ñ –∫–∞—Ä—Ç–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤
            product_cards = soup.find_all('div', class_='jet-listing-grid__item')
            
            products = []
            for card in product_cards:
                try:
                    # –ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É
                    title_elem = card.find('h3', class_='elementor-heading-title')
                    if not title_elem:
                        continue
                    
                    title_link = title_elem.find('a')
                    title = title_link.text.strip() if title_link else title_elem.text.strip()
                    product_url = title_link['href'] if title_link and title_link.get('href') else None
                    
                    if not product_url or product_url in self.processed_urls:
                        continue
                    
                    # –û–ø–∏—Å —Ç–æ–≤–∞—Ä—É
                    description_elem = card.find('div', class_='description')
                    description = ''
                    if description_elem:
                        desc_container = description_elem.find('div', class_='elementor-widget-container')
                        if desc_container:
                            # –ó–±–∏—Ä–∞—î–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤
                            paragraphs = desc_container.find_all('p')
                            description = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    
                    # –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Ç–æ–≤–∞—Ä—É (–ø—Ä–µ–≤—å—é)
                    img_elem = card.find('img', class_='lazyloaded')
                    if not img_elem:
                        img_elem = card.find('img')
                    
                    feature_photo = ''
                    if img_elem:
                        feature_photo = img_elem.get('src') or img_elem.get('data-lazy-src', '')
                    
                    products.append({
                        'url': product_url,
                        'title': title,
                        'description': description,
                        'feature_photo': feature_photo,
                        'collection': collection_name
                    })
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∫–∞—Ä—Ç–∫–∏ —Ç–æ–≤–∞—Ä—É: {e}")
                    continue
            
            print(f"‚ú® –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ —É –∫–æ–ª–µ–∫—Ü—ñ—ó: {len(products)}")
            return products
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
            return []
    
    def parse_product_detail(self, url):
        """–ü–∞—Ä—Å–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–æ–≤–∞—Ä—É —Ç–∞ –æ—Ç—Ä–∏–º—É—î –≥–∞–ª–µ—Ä–µ—é —ñ —Ç–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                self.driver.get(url)
                time.sleep(3)
                
                # –ü—Ä–æ–∫—Ä—É—á—É—î–º–æ –¥–æ –≥–∞–ª–µ—Ä–µ—ó
                self.driver.execute_script("window.scrollTo(0, 800);")
                time.sleep(1)
                
                html = self.driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # –®—É–∫–∞—î–º–æ –≤—Å—ñ —Å–ª–∞–π–¥–∏ —É —Å–≤–∞–π–ø–µ—Ä—ñ
                gallery_images = []
                swiper_slides = soup.find_all('div', class_='swiper-slide')
                
                for slide in swiper_slides[:3]:  # –ë–µ—Ä–µ–º–æ –º–∞–∫—Å–∏–º—É–º 3 –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                    # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
                    if 'swiper-slide-duplicate' in slide.get('class', []):
                        continue
                    
                    img = slide.find('img', class_='swiper-slide-image')
                    if img:
                        img_url = img.get('data-lazy-src') or img.get('src', '')
                        if img_url and img_url.startswith('http'):
                            gallery_images.append(img_url)
                
                # –î–æ–ø–æ–≤–Ω—é—î–º–æ –¥–æ 3 –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
                while len(gallery_images) < 3:
                    gallery_images.append('')
                
                # –®—É–∫–∞—î–º–æ —Ç–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ
                surface_type = ''
                format_rows = soup.find_all('div', class_='jedv-enabled--yes')
                
                for row in format_rows:
                    # –®—É–∫–∞—î–º–æ –≤—Å—ñ heading –µ–ª–µ–º–µ–Ω—Ç–∏ –≤ —Ä—è–¥–∫—É
                    headings = row.find_all('div', class_='elementor-widget-heading')
                    
                    # –¢—Ä–µ—Ç—ñ–π –µ–ª–µ–º–µ–Ω—Ç - —Ü–µ —Ç–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ
                    if len(headings) >= 3:
                        surface_span = headings[2].find('span', class_='elementor-heading-title')
                        if surface_span:
                            surface_type = surface_span.text.strip()
                            break
                
                # –ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–∏–ø—ñ–≤ –ø–æ–≤–µ—Ä—Ö–æ–Ω—å
                surface_translations = {
                    'Polished': '–ü–æ–ª—ñ—Ä–æ–≤–∞–Ω–∞',
                    'Matt': '–ú–∞—Ç–æ–≤–∞',
                    'Lappato': '–õ–∞–ø–ø–∞—Ç–æ–≤–∞–Ω–∞',
                    'Feel': '–ù–∞—Ç—É—Ä–∞–ª—å–Ω–∞',
                    'Natural': '–ù–∞—Ç—É—Ä–∞–ª—å–Ω–∞',
                    'Velvet': '–û–∫—Å–∞–º–∏—Ç–æ–≤–∞',
                    'Structured': '–°—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∞'
                }
                
                # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ —è–∫—â–æ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–µ—Ä–µ–∫–ª–∞–¥
                translated_surfaces = []
                if surface_type:
                    for surf in surface_type.split(','):
                        surf = surf.strip()
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –ø–µ—Ä–µ–∫–ª–∞–¥
                        translated = surface_translations.get(surf, surf)
                        translated_surfaces.append(translated)
                    
                    surface_type = ', '.join(translated_surfaces)
                
                return {
                    'gallery': gallery_images[:3],
                    'surface_type': surface_type
                }
                
            except WebDriverException as e:
                if attempt < max_retries - 1:
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è, —Å–ø—Ä–æ–±–∞ {attempt + 2}/{max_retries}...")
                    self.restart_driver()
                    time.sleep(2)
                else:
                    print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—å –æ–±—Ä–æ–±–∏—Ç–∏ {url}: {e}")
                    return {
                        'gallery': ['', '', ''],
                        'surface_type': ''
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {url}: {e}")
                return {
                    'gallery': ['', '', ''],
                    'surface_type': ''
                }
    
    def save_product_to_excel(self, product_data):
        """–î–æ–¥–∞—î –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –¥–æ Excel —Ñ–∞–π–ª—É"""
        try:
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {product_data.get('Title', '–ë–µ–∑ –Ω–∞–∑–≤–∏')}")
            
            # –Ø–∫—â–æ —Ñ–∞–π–ª —ñ—Å–Ω—É—î, –¥–æ–ø–∏—Å—É—î–º–æ
            if os.path.exists(self.output_file):
                from openpyxl import load_workbook
                
                wb = load_workbook(self.output_file)
                ws = wb['Products']
                
                ws.append([
                    product_data['Brand'],
                    product_data['Category'],
                    product_data['Collection'],
                    product_data['Title'],
                    product_data['Description'],
                    product_data['Feature photo'],
                    product_data['Type'],
                    product_data['Gallery1'],
                    product_data['Gallery2'],
                    product_data['Gallery3']
                ])
                
                wb.save(self.output_file)
                wb.close()
                print(f"   ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ!")
            else:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª
                df = pd.DataFrame([product_data])
                with pd.ExcelWriter(self.output_file, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Products')
                    
                    worksheet = writer.sheets['Products']
                    column_widths = {
                        'A': 15,  # Brand
                        'B': 25,  # Category
                        'C': 20,  # Collection
                        'D': 30,  # Title
                        'E': 60,  # Description
                        'F': 50,  # Feature photo
                        'G': 20,  # Type
                        'H': 50,  # Gallery1
                        'I': 50,  # Gallery2
                        'J': 50,  # Gallery3
                    }
                    for col, width in column_widths.items():
                        worksheet.column_dimensions[col].width = width
                        
                print(f"   ‚úÖ –§–∞–π–ª —Å—Ç–≤–æ—Ä–µ–Ω–æ: {self.output_file}")
                        
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {e}")
            import traceback
            traceback.print_exc()
    
    def parse_all(self, main_url):
        """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –ø–∞—Ä—Å–∏–Ω–≥—É"""
        print("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É Ascale –∫–µ—Ä–∞–º–æ–≥—Ä–∞–Ω—ñ—Ç—É\n")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ –∫–æ–ª–µ–∫—Ü—ñ—ó
        collections = self.get_collection_urls(main_url)
        
        if not collections:
            print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó!")
            return
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–Ω—É –∫–æ–ª–µ–∫—Ü—ñ—é
        for collection in collections:
            print(f"\n{'='*60}")
            print(f"üìÇ –ö–æ–ª–µ–∫—Ü—ñ—è: {collection['name']}")
            print(f"üîó URL: {collection['url']}")
            print(f"{'='*60}")
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–≤–∞—Ä–∏ –∑ –∫–æ–ª–µ–∫—Ü—ñ—ó
            products = self.parse_collection_page(collection['url'], collection['name'])
            
            if not products:
                print(f"‚ö†Ô∏è –ù–µ–º–∞—î —Ç–æ–≤–∞—Ä—ñ–≤ —É –∫–æ–ª–µ–∫—Ü—ñ—ó '{collection['name']}'")
                continue
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω —Ç–æ–≤–∞—Ä
            print(f"\nüì¶ –û–±—Ä–æ–±–∫–∞ —Ç–æ–≤–∞—Ä—ñ–≤...")
            
            for product in tqdm(products, desc=collection['name']):
                if product['url'] and product['url'] not in self.processed_urls:
                    try:
                        print(f"\nüîç –û–±—Ä–æ–±–∫–∞: {product['title']}")
                        
                        # –û—Ç—Ä–∏–º—É—î–º–æ –≥–∞–ª–µ—Ä–µ—é —Ç–∞ —Ç–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                        details = self.parse_product_detail(product['url'])
                        
                        # –§–æ—Ä–º—É—î–º–æ –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É
                        product_data = {
                            'Brand': self.brand,
                            'Category': self.category,
                            'Collection': product['collection'],
                            'Title': product['title'],
                            'Description': product['description'],
                            'Feature photo': product['feature_photo'],
                            'Type': details['surface_type'],
                            'Gallery1': details['gallery'][0],
                            'Gallery2': details['gallery'][1],
                            'Gallery3': details['gallery'][2]
                        }
                        
                        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–æ–≤–∞—Ä
                        self.save_product_to_excel(product_data)
                        
                        # –î–æ–¥–∞—î–º–æ –¥–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö
                        self.processed_urls.add(product['url'])
                        
                        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
                        if len(self.processed_urls) % 5 == 0:
                            self.save_progress()
                        
                        time.sleep(0.5)
                        
                    except Exception as e:
                        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ç–æ–≤–∞—Ä—É: {e}")
                        self.save_progress()
                        continue
        
        # –§—ñ–Ω–∞–ª—å–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        self.save_progress()
        
        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –í—Å—å–æ–≥–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {len(self.processed_urls)}")
        print(f"üíæ –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {self.output_file}")
    
    def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î –±—Ä–∞—É–∑–µ—Ä"""
        if self.driver:
            self.driver.quit()


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    parser = AscaleParser(output_file='ascale_ceramic.xlsx')
    
    try:
        # URL –≥–æ–ª–æ–≤–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ–π
        main_url = 'https://www.ascale.es/en/collections/'
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–∞—Ä—Å–∏–Ω–≥
        parser.parse_all(main_url)
        
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –∑—É–ø–∏–Ω–µ–Ω–æ")
        print("üíæ –ü—Ä–æ–≥—Ä–µ—Å –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        print("üíæ –ü—Ä–æ–≥—Ä–µ—Å –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    finally:
        parser.save_progress()
        parser.close()


if __name__ == "__main__":
    main()