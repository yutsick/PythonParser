"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç—É sapienstone.com - –ö–µ—Ä–∞–º–æ–≥—Ä–∞–Ω—ñ—Ç
–ó–±–∏—Ä–∞—î –¥–∞–Ω—ñ –∑ –∫–æ–ª–µ–∫—Ü—ñ–π –±—Ä–µ–Ω–¥—É Sapienstone
–†–µ–∑—É–ª—å—Ç–∞—Ç: sapienstone_ceramic.xlsx
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
import time
from tqdm import tqdm
import os
import json

class SapienstoneParser:
    def __init__(self, output_file='sapienstone_ceramic.xlsx'):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.output_file = output_file
        self.progress_file = 'progress_sapienstone.json'
        self.driver = None
        self.wait = None
        self.processed_urls = set()
        self.brand = 'Sapienstone'
        self.category = '–ö–µ—Ä–∞–º–æ–≥—Ä–∞–Ω—ñ—Ç'
        self.base_url = 'https://www.sapienstone.com'
        self.init_driver()
        self.load_progress()
        
    def init_driver(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –¥—Ä–∞–π–≤–µ—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
        options = webdriver.ChromeOptions()
        # –ó–∞–∫–æ–º–µ–Ω—Ç—É–π—Ç–µ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—è–¥–æ–∫, —è–∫—â–æ —Ö–æ—á–µ—Ç–µ –±–∞—á–∏—Ç–∏ –±—Ä–∞—É–∑–µ—Ä
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.wait = WebDriverWait(self.driver, 15)
            print("‚úÖ –ë—Ä–∞—É–∑–µ—Ä —É—Å–ø—ñ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–æ")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø—É—Å–∫—É –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            raise
    
    def load_progress(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø—Ä–æ–≥—Ä–µ—Å –∑ —Ñ–∞–π–ª—É"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –ø—Ä–æ–≥—Ä–µ—Å: {len(self.processed_urls)} —Ç–æ–≤–∞—Ä—ñ–≤ –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω–æ")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—å –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å: {e}")
                self.processed_urls = set()
        else:
            print("üÜï –ü–æ—á–∞—Ç–æ–∫ –Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É")
    
    def save_progress(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–≥—Ä–µ—Å —É —Ñ–∞–π–ª"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls)
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É: {e}")
    
    def restart_driver(self):
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –¥—Ä–∞–π–≤–µ—Ä–∞ –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö"""
        print("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞...")
        try:
            if self.driver:
                self.driver.quit()
        except:
            pass
        
        time.sleep(3)
        self.init_driver()
    
    def parse_catalog_page(self, catalog_url):
        """–ü–∞—Ä—Å–∏—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É –∫–∞—Ç–∞–ª–æ–≥—É —Ç–∞ –æ—Ç—Ä–∏–º—É—î –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏"""
        print("üåê –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–∞–ª–æ–≥—É...")
        
        try:
            self.driver.get(catalog_url)
            time.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # –®—É–∫–∞—î–º–æ –≤—Å—ñ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏ —Ç–æ–≤–∞—Ä—ñ–≤
            product_containers = soup.find_all('div', class_='product-container')
            
            print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {len(product_containers)}")
            
            products = []
            for container in product_containers:
                try:
                    # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ç–æ–≤–∞—Ä
                    link = container.find('a')
                    if not link or not link.get('href'):
                        continue
                    
                    product_url = self.base_url + link['href']
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω—ñ
                    if product_url in self.processed_urls:
                        continue
                    
                    # –ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É
                    title = ''
                    p_tag = container.find('p')
                    if p_tag:
                        strong = p_tag.find('strong')
                        if strong:
                            title = strong.text.strip()
                    
                    # –¢–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ (Cashmere, —Ç–æ—â–æ)
                    surface_type = ''
                    if p_tag:
                        i_tag = p_tag.find('i')
                        if i_tag:
                            surface_type = i_tag.text.strip()
                    
                    # –ö–∞—Ä—Ç–∏–Ω–∫–∞ (–ø—Ä–µ–≤—å—é)
                    feature_photo = ''
                    img = container.find('img')
                    if img and img.get('src'):
                        feature_photo = self.base_url + img['src']
                    
                    products.append({
                        'url': product_url,
                        'title': title,
                        'surface_type': surface_type,
                        'feature_photo': feature_photo
                    })
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: {e}")
                    continue
            
            print(f"‚ú® –ù–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏: {len(products)}")
            return products
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–∞–ª–æ–≥—É: {e}")
            return []
    
    def parse_product_detail(self, url):
        """–ü–∞—Ä—Å–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–æ–≤–∞—Ä—É —Ç–∞ –æ—Ç—Ä–∏–º—É—î –≥–∞–ª–µ—Ä–µ—é"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                self.driver.get(url)
                time.sleep(3)
                
                # –ü—Ä–æ–∫—Ä—É—á—É—î–º–æ –¥–æ —Å–ª–∞–π–¥–µ—Ä–∞
                self.driver.execute_script("window.scrollTo(0, 500);")
                time.sleep(1)
                
                html = self.driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # –®—É–∫–∞—î–º–æ slick-slider
                gallery_images = []
                slick_track = soup.find('div', class_='slick-track')
                
                if slick_track:
                    slides = slick_track.find_all('div', class_='slick-slide')
                    
                    for slide in slides[:3]:  # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—ñ 3 —Å–ª–∞–π–¥–∏
                        # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤–µ–ª–∏–∫–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                        link = slide.find('a')
                        if link and link.get('href'):
                            # –ë–µ—Ä–µ–º–æ big –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞ –Ω–µ thumb
                            img_url = self.base_url + link['href']
                            gallery_images.append(img_url)
                
                # –î–æ–ø–æ–≤–Ω—é—î–º–æ –¥–æ 3 –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
                while len(gallery_images) < 3:
                    gallery_images.append('')
                
                return gallery_images[:3]
                
            except WebDriverException as e:
                if attempt < max_retries - 1:
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è, —Å–ø—Ä–æ–±–∞ {attempt + 2}/{max_retries}...")
                    self.restart_driver()
                    time.sleep(2)
                else:
                    print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—å –æ–±—Ä–æ–±–∏—Ç–∏ {url}: {e}")
                    return ['', '', '']
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {url}: {e}")
                return ['', '', '']
    
    def translate_surface_type(self, surface_type):
        """–ü–µ—Ä–µ–∫–ª–∞–¥–∞—î —Ç–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É"""
        translations = {
            'Cashmere': '–ö–∞—à–µ–º—ñ—Ä',
            'Polished': '–ü–æ–ª—ñ—Ä–æ–≤–∞–Ω–∞',
            'Matt': '–ú–∞—Ç–æ–≤–∞',
            'Silk': '–®–æ–≤–∫–æ–≤–∞',
            'Natural': '–ù–∞—Ç—É—Ä–∞–ª—å–Ω–∞',
            'Honed': '–®–ª—ñ—Ñ–æ–≤–∞–Ω–∞',
            'Structured': '–°—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∞'
        }
        
        return translations.get(surface_type, surface_type)
    
    def save_product_to_excel(self, product_data):
        """–î–æ–¥–∞—î –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –¥–æ Excel —Ñ–∞–π–ª—É"""
        try:
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {product_data.get('Title', '–ë–µ–∑ –Ω–∞–∑–≤–∏')}")
            
            # –Ø–∫—â–æ —Ñ–∞–π–ª —ñ—Å–Ω—É—î, –¥–æ–ø–∏—Å—É—î–º–æ
            if os.path.exists(self.output_file):
                from openpyxl import load_workbook
                
                wb = load_workbook(self.output_file)
                ws = wb['Products']
                
                ws.append([
                    product_data['Brand'],
                    product_data['Category'],
                    product_data['Title'],
                    product_data['Type'],
                    product_data['Feature photo'],
                    product_data['Gallery1'],
                    product_data['Gallery2'],
                    product_data['Gallery3']
                ])
                
                wb.save(self.output_file)
                wb.close()
                print(f"   ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ!")
            else:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª
                df = pd.DataFrame([product_data])
                with pd.ExcelWriter(self.output_file, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Products')
                    
                    worksheet = writer.sheets['Products']
                    column_widths = {
                        'A': 20,  # Brand
                        'B': 25,  # Category
                        'C': 30,  # Title
                        'D': 20,  # Type
                        'E': 50,  # Feature photo
                        'F': 50,  # Gallery1
                        'G': 50,  # Gallery2
                        'H': 50,  # Gallery3
                    }
                    for col, width in column_widths.items():
                        worksheet.column_dimensions[col].width = width
                        
                print(f"   ‚úÖ –§–∞–π–ª —Å—Ç–≤–æ—Ä–µ–Ω–æ: {self.output_file}")
                        
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {e}")
            import traceback
            traceback.print_exc()
    
    def parse_all(self, catalog_url):
        """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –ø–∞—Ä—Å–∏–Ω–≥—É"""
        print("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É Sapienstone –∫–µ—Ä–∞–º–æ–≥—Ä–∞–Ω—ñ—Ç—É\n")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –∑ –∫–∞—Ç–∞–ª–æ–≥—É
        products = self.parse_catalog_page(catalog_url)
        
        if not products:
            print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤!")
            return
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω —Ç–æ–≤–∞—Ä
        print(f"\nüì¶ –û–±—Ä–æ–±–∫–∞ —Ç–æ–≤–∞—Ä—ñ–≤...")
        
        for product in tqdm(products, desc="–ü—Ä–æ–≥—Ä–µ—Å"):
            if product['url'] and product['url'] not in self.processed_urls:
                try:
                    print(f"\nüîç –û–±—Ä–æ–±–∫–∞: {product['title']}")
                    
                    # –û—Ç—Ä–∏–º—É—î–º–æ –≥–∞–ª–µ—Ä–µ—é –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                    gallery = self.parse_product_detail(product['url'])
                    
                    # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ —Ç–∏–ø –ø–æ–≤–µ—Ä—Ö–Ω—ñ
                    surface_type_ua = self.translate_surface_type(product['surface_type'])
                    
                    # –§–æ—Ä–º—É—î–º–æ –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É
                    product_data = {
                        'Brand': self.brand,
                        'Category': self.category,
                        'Title': product['title'],
                        'Type': surface_type_ua,
                        'Feature photo': product['feature_photo'],
                        'Gallery1': gallery[0],
                        'Gallery2': gallery[1],
                        'Gallery3': gallery[2]
                    }
                    
                    print(f"   üìã –î–∞–Ω—ñ –∑—ñ–±—Ä–∞–Ω–æ: Type={surface_type_ua}")
                    
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–æ–≤–∞—Ä
                    self.save_product_to_excel(product_data)
                    
                    # –î–æ–¥–∞—î–º–æ –¥–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö
                    self.processed_urls.add(product['url'])
                    
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
                    if len(self.processed_urls) % 5 == 0:
                        self.save_progress()
                    
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ç–æ–≤–∞—Ä—É: {e}")
                    self.save_progress()
                    continue
        
        # –§—ñ–Ω–∞–ª—å–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        self.save_progress()
        
        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –í—Å—å–æ–≥–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {len(self.processed_urls)}")
        print(f"üíæ –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {self.output_file}")
    
    def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î –±—Ä–∞—É–∑–µ—Ä"""
        if self.driver:
            self.driver.quit()


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    parser = SapienstoneParser(output_file='sapienstone_ceramic.xlsx')
    
    try:
        # URL –∫–∞—Ç–∞–ª–æ–≥—É
        catalog_url = 'https://www.sapienstone.com/collections'
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–∞—Ä—Å–∏–Ω–≥
        parser.parse_all(catalog_url)
        
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –∑—É–ø–∏–Ω–µ–Ω–æ")
        print("üíæ –ü—Ä–æ–≥—Ä–µ—Å –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        print("üíæ –ü—Ä–æ–≥—Ä–µ—Å –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    finally:
        parser.save_progress()
        parser.close()


if __name__ == "__main__":
    main()